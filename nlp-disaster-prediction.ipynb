{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection, preprocessing, decomposition\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\n\n#####\n\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom keras.layers.normalization import BatchNormalization\n\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config Completer.use_jedi = False","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first import our training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train = pd.read_csv(\"train.csv\")\ntweets_test = pd.read_csv(\"test.csv\")","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.head(3)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n\n   target  \n0       1  \n1       1  \n2       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Let's look at an example of each kind of tweets, as 1 means disaster and 0 means no disaster"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No disaster tweet: \",tweets_train[tweets_train.target == 0].text.values[0])\nprint(\"Disaster tweet: \",tweets_train[tweets_train.target == 1].text.values[0])","execution_count":7,"outputs":[{"output_type":"stream","text":"No disaster tweet:  What's up man?\nDisaster tweet:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.dropna()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"         id  keyword                       location  \\\n31       48   ablaze                     Birmingham   \n32       49   ablaze  Est. September 2012 - Bristol   \n33       50   ablaze                         AFRICA   \n34       52   ablaze               Philadelphia, PA   \n35       53   ablaze                     London, UK   \n...     ...      ...                            ...   \n7575  10826  wrecked                             TN   \n7577  10829  wrecked         #NewcastleuponTyne #UK   \n7579  10831  wrecked              Vancouver, Canada   \n7580  10832  wrecked                        London    \n7581  10833  wrecked                        Lincoln   \n\n                                                   text  target  \n31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n32    We always try to bring the heavy. #metal #RT h...       0  \n33    #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n34                   Crying out for more! Set me ablaze       0  \n35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n...                                                 ...     ...  \n7575  On the bright side I wrecked http://t.co/uEa0t...       0  \n7577  @widda16 ... He's gone. You can relax. I thoug...       0  \n7579  Three days off from work and they've pretty mu...       0  \n7580  #FX #forex #trading Cramer: Iger's 3 words tha...       0  \n7581  @engineshed Great atmosphere at the British Li...       0  \n\n[5080 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31</th>\n      <td>48</td>\n      <td>ablaze</td>\n      <td>Birmingham</td>\n      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>49</td>\n      <td>ablaze</td>\n      <td>Est. September 2012 - Bristol</td>\n      <td>We always try to bring the heavy. #metal #RT h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>50</td>\n      <td>ablaze</td>\n      <td>AFRICA</td>\n      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>52</td>\n      <td>ablaze</td>\n      <td>Philadelphia, PA</td>\n      <td>Crying out for more! Set me ablaze</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>53</td>\n      <td>ablaze</td>\n      <td>London, UK</td>\n      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7575</th>\n      <td>10826</td>\n      <td>wrecked</td>\n      <td>TN</td>\n      <td>On the bright side I wrecked http://t.co/uEa0t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7577</th>\n      <td>10829</td>\n      <td>wrecked</td>\n      <td>#NewcastleuponTyne #UK</td>\n      <td>@widda16 ... He's gone. You can relax. I thoug...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7579</th>\n      <td>10831</td>\n      <td>wrecked</td>\n      <td>Vancouver, Canada</td>\n      <td>Three days off from work and they've pretty mu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7580</th>\n      <td>10832</td>\n      <td>wrecked</td>\n      <td>London</td>\n      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7581</th>\n      <td>10833</td>\n      <td>wrecked</td>\n      <td>Lincoln</td>\n      <td>@engineshed Great atmosphere at the British Li...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5080 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(7613, 5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.info()","execution_count":10,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of tweets related to a disaster: \",tweets_train[tweets_train.target == 1].target.count())\nprint(\"Number of tweets non-related to a disaster: \",tweets_train[tweets_train.target == 0].target.count())","execution_count":11,"outputs":[{"output_type":"stream","text":"Number of tweets related to a disaster:  3271\nNumber of tweets non-related to a disaster:  4342\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 7613 tweets with an Id, the text of the tweets and the target variable (1 or 0). 7552/7613 involve key words and 5080/7613 involve a location for the tweet. We have also seen that 3271 tweets in our data set are related to a disaster while 4342 are not."},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"We are only going to work on the text(x) and target(y) variables. <br/>\nFirst we are going to use TF-IDF."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data splitting\nx_train, x_valid, y_train, y_valid = train_test_split(tweets_train.text.values,tweets_train.target.values , \n                                                  stratify=tweets_train.target.values, \n                                                  random_state=29, \n                                                  test_size=0.2, shuffle=True)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (x_train.shape)\nprint (x_valid.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"(6090,)\n(1523,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Here, we are going to instanciate the TF-IDF vectorizer which will be later applied to our data. We are going to use those features, they work almost everytime."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that our vectorizer is instanciated, we can fit TF-IDF to both our train and test data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv.fit(list(x_train)+list(x_valid))\n\ntfv_x_train = tfv.transform(x_train)\ntfv_x_valid = tfv.transform(x_valid)","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is ready we are going to apply the following models:\n* Logistic Regression\n* Ridge Classifier\n* Support Vector Machine\n* Naive Bayes <br/>\n\n\nThe evaluation metric we are going to use is the **f1 score**.\n\nPS: We are not going to focus on the hyperparameters."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=1)\nlr.fit(tfv_x_train,y_train)\npredictions = lr.predict(tfv_x_valid)\nf1_score(predictions,y_valid)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"0.7392795883361921"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rc = RidgeClassifier()\nrc.fit(tfv_x_train,y_train)\npredictions = rc.predict(tfv_x_valid)\nf1_score(predictions,y_valid)","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"0.7457072771872445"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine (SVM)\n\nSVMs take a lot of time so it would be better to reduce the dimensionality of our data using decomposition.TruncatedSVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD(n_components=200)\nsvd.fit(tfv_x_train)\nxtrain_svd = svd.transform(tfv_x_train)\nxvalid_svd = svd.transform(tfv_x_valid)\n","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we will have to standardize our data before passing it into the SVM model since SVMs are influenced by the scale of the data, so having our data with a mean of 0 and a variance of 1 would remove all bias."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\nscaler.fit(xtrain_svd)\nx_train_scaled = scaler.transform(xtrain_svd)\nx_valid_scaled = scaler.transform(xvalid_svd)\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC()\nsvm.fit(x_train_scaled,y_train)\npredictions = svm.predict(x_valid_scaled)\nf1_score(predictions,y_valid)","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"0.7114210985178727"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(tfv_x_train,y_train)\npredictions = nb.predict(tfv_x_valid)\nf1_score(predictions,y_valid)","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"0.7099099099099099"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Count Vectorizer<br/>\nNow that we are done with the TF-IDF method, we are going to do the same thing with the Count Vectorizer instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv.fit(list(x_train)+list(x_valid))\n\nctv_x_train = ctv.transform(x_train)\nctv_x_valid = ctv.transform(x_valid)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(ctv_x_train,y_train)\npredictions = lr.predict(ctv_x_valid)\nf1_score(predictions,y_valid)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"0.7293103448275862"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"rc.fit(ctv_x_train,y_train)\npredictions = rc.predict(ctv_x_valid)\nf1_score(predictions,y_valid)","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"0.7271171941830625"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.fit(ctv_x_train,y_train)\npredictions = nb.predict(ctv_x_valid)\nf1_score(predictions,y_valid)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"0.7571318427139553"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD(n_components=200)\nsvd.fit(ctv_x_train)\nxtrain_svd = svd.transform(ctv_x_train)\nxvalid_svd = svd.transform(ctv_x_valid)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(xtrain_svd)\nx_train_scaled = scaler.transform(xtrain_svd)\nx_valid_scaled = scaler.transform(xvalid_svd)\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.fit(x_train_scaled,y_train)\npredictions = svm.predict(x_valid_scaled)\nf1_score(predictions,y_valid)","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"0.6685446009389672"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 1000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)\ntokenize.fit_on_texts(x_train) # only fit on train","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_tkn = tokenize.texts_to_matrix(x_train)\nx_valid_tkn = tokenize.texts_to_matrix(x_valid)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = np.max(y_train) + 1\ny_train_tkn = utils.to_categorical(y_train, num_classes)\ny_valid_tkn = utils.to_categorical(y_valid, num_classes)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train_tkn, y_train_tkn,\n                    batch_size=batch_size,\n                    epochs=5,\n                    verbose=1)","execution_count":35,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n191/191 [==============================] - 1s 3ms/step - loss: 0.5906 - accuracy: 0.6835\nEpoch 2/5\n191/191 [==============================] - 1s 3ms/step - loss: 0.3956 - accuracy: 0.8248\nEpoch 3/5\n191/191 [==============================] - 1s 3ms/step - loss: 0.3261 - accuracy: 0.8626\nEpoch 4/5\n191/191 [==============================] - 1s 3ms/step - loss: 0.2682 - accuracy: 0.8943\nEpoch 5/5\n191/191 [==============================] - 1s 3ms/step - loss: 0.2218 - accuracy: 0.9240\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_valid_tkn, y_valid_tkn,\n                       batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","execution_count":36,"outputs":[{"output_type":"stream","text":"48/48 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8070\nTest accuracy: 0.8069599270820618\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(x_valid_tkn)\nf1_score(predictions,y_valid)","execution_count":39,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n","name":"stderr"},{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"0.7621359223300971"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}