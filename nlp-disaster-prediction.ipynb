{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection, preprocessing, decomposition\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\n\n#####\n\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom keras.layers.normalization import BatchNormalization\n\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config Completer.use_jedi = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first import our training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train = pd.read_csv(\"train.csv\")\ntweets_test = pd.read_csv(\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at an example of each kind of tweets, as 1 means disaster and 0 means no disaster"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No disaster tweet: \",tweets_train[tweets_train.target == 0].text.values[0])\nprint(\"Disaster tweet: \",tweets_train[tweets_train.target == 1].text.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of tweets related to a disaster: \",tweets_train[tweets_train.target == 1].target.count())\nprint(\"Number of tweets non-related to a disaster: \",tweets_train[tweets_train.target == 0].target.count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 7613 tweets with an Id, the text of the tweets and the target variable (1 or 0). 7552/7613 involve key words and 5080/7613 involve a location for the tweet. We have also seen that 3271 tweets in our data set are related to a disaster while 4342 are not."},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"We are only going to work on the text(x) and target(y) variables. <br/>\nFirst we are going to use TF-IDF."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data splitting\nx_train, x_valid, y_train, y_valid = train_test_split(tweets_train.text.values,tweets_train.target.values , \n                                                  stratify=tweets_train.target.values, \n                                                  random_state=29, \n                                                  test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (x_train.shape)\nprint (x_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we are going to instanciate the TF-IDF vectorizer which will be later applied to our data. We are going to use those features, they work almost everytime."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that our vectorizer is instanciated, we can fit TF-IDF to both our train and test data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv.fit(list(x_train)+list(x_valid))\n\ntfv_x_train = tfv.transform(x_train)\ntfv_x_valid = tfv.transform(x_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is ready we are going to apply the following models:\n* Logistic Regression\n* Ridge Classifier\n* Support Vector Machine\n* Naive Bayes <br/>\n\n\nThe evaluation metric we are going to use is the **f1 score**.\n\nPS: We are not going to focus on the hyperparameters."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=1)\nlr.fit(tfv_x_train,y_train)\npredictions = lr.predict(tfv_x_valid)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rc = RidgeClassifier()\nrc.fit(tfv_x_train,y_train)\npredictions = rc.predict(tfv_x_valid)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine (SVM)\n\nSVMs take a lot of time so it would be better to reduce the dimensionality of our data using decomposition.TruncatedSVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD(n_components=200)\nsvd.fit(tfv_x_train)\nxtrain_svd = svd.transform(tfv_x_train)\nxvalid_svd = svd.transform(tfv_x_valid)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we will have to standardize our data before passing it into the SVM model since SVMs are influenced by the scale of the data, so having our data with a mean of 0 and a variance of 1 would remove all bias."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\nscaler.fit(xtrain_svd)\nx_train_scaled = scaler.transform(xtrain_svd)\nx_valid_scaled = scaler.transform(xvalid_svd)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC()\nsvm.fit(x_train_scaled,y_train)\npredictions = svm.predict(x_valid_scaled)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(tfv_x_train,y_train)\npredictions = nb.predict(tfv_x_valid)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Count Vectorizer<br/>\nNow that we are done with the TF-IDF method, we are going to do the same thing with the Count Vectorizer instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv.fit(list(x_train)+list(x_valid))\n\nctv_x_train = ctv.transform(x_train)\nctv_x_valid = ctv.transform(x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(ctv_x_train,y_train)\npredictions = lr.predict(ctv_x_valid)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rc.fit(ctv_x_train,y_train)\npredictions = rc.predict(ctv_x_valid)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.fit(ctv_x_train,y_train)\npredictions = nb.predict(ctv_x_valid)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD(n_components=200)\nsvd.fit(ctv_x_train)\nxtrain_svd = svd.transform(ctv_x_train)\nxvalid_svd = svd.transform(ctv_x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(xtrain_svd)\nx_train_scaled = scaler.transform(xtrain_svd)\nx_valid_scaled = scaler.transform(xvalid_svd)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.fit(x_train_scaled,y_train)\npredictions = svm.predict(x_valid_scaled)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 1000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)\ntokenize.fit_on_texts(x_train) # only fit on train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_tkn = tokenize.texts_to_matrix(x_train)\nx_valid_tkn = tokenize.texts_to_matrix(x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = np.max(y_train) + 1\ny_train_tkn = utils.to_categorical(y_train, num_classes)\ny_valid_tkn = utils.to_categorical(y_valid, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2\n\n# Build the model\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train_tkn, y_train_tkn,\n                    batch_size=batch_size,\n                    epochs=5,\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_valid_tkn, y_valid_tkn,\n                       batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(x_valid_tkn)\nf1_score(predictions,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}